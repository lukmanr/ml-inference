{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF Serving Basics\n",
    "\n",
    "## Install docker (assumes Deep Learning VM)\n",
    "\n",
    "```\n",
    "sudo apt-get remove docker docker-engine docker.io\n",
    "\n",
    "sudo apt-get install \\\n",
    "     apt-transport-https \\\n",
    "     ca-certificates \\\n",
    "     curl \\\n",
    "     gnupg2 \\\n",
    "     software-properties-common\n",
    "\n",
    "curl -fsSL https://download.docker.com/linux/debian/gpg | sudo apt-key add -\n",
    "\n",
    "sudo add-apt-repository \\\n",
    "   \"deb [arch=amd64] https://download.docker.com/linux/debian \\\n",
    "   $(lsb_release -cs) \\\n",
    "   stable\"\n",
    "\n",
    "sudo apt-get update\n",
    "\n",
    "sudo apt-get install -y docker-ce\n",
    "\n",
    "sudo usermod -a -G docker $USER\n",
    "\n",
    "```\n",
    "\n",
    "* **Exit your shell and relogin**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pull TF Serving container\n",
    "\n",
    "```\n",
    "docker pull tensorflow/serving\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Serving example\n",
    "\n",
    "Once you have pulled the serving image, you can try serving an example model.\n",
    "\n",
    "We will use a toy model called Half Plus Three, which will predict values 0.5 * x + 3 for the values we provide for prediction.\n",
    "\n",
    "To get this model, first clone the TensorFlow Serving repo:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "mkdir -p /tmp/tfserving\n",
    "cd /tmp/tfserving\n",
    "git clone https://github.com/tensorflow/serving\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, run the TensorFlow Serving container pointing it to this model and opening the REST API port (8501):\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "docker run -p 8501:8501 \\\n",
    "-v /tmp/tfserving/serving/tensorflow_serving/servables/tensorflow/testdata/saved_model_half_plus_three:/models/half_plus_three \\\n",
    "-e MODEL_NAME=half_plus_three -t tensorflow/serving &\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will run the docker container and launch the TensorFlow Serving Model Server, bind the REST API port 8501, and map our desired model from our host to where models are expected in the container. We also pass the name of the model as an environment variable, which will be important when we query the model.\n",
    "\n",
    "To query the model using the predict API, you can run:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "curl -d '{\"instances\": [1.0, 2.0, 5.0]}' -X POST http://localhost:8601/v1/models/half_plus_three:predict\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
